#!/usr/bin/env python3
"""
AIæœç´¢æµé‡ä¼˜åŒ–æ¨¡å—
ä¸“ä¸ºæå‡å†…å®¹åœ¨AIæœç´¢å¼•æ“ä¸­çš„å¼•ç”¨ç‡å’Œå¯è§åº¦è€Œè®¾è®¡
"""

import json
import re
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import nltk
from textstat import flesch_reading_ease, flesch_kincaid_grade
import spacy
from dataclasses import dataclass
from enum import Enum
import openai
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


class ContentType(Enum):
    """å†…å®¹ç±»å‹æšä¸¾"""
    FAQ = "faq"
    HOW_TO = "how_to"
    COMPARISON = "comparison"
    PRODUCT_GUIDE = "product_guide"
    TROUBLESHOOTING = "troubleshooting"


@dataclass
class AIOptimizationResult:
    """AIä¼˜åŒ–ç»“æœæ•°æ®ç±»"""
    original_content: str
    optimized_content: str
    ai_readiness_score: float
    predicted_citation_rate: float
    semantic_clarity_score: float
    structure_score: float
    authority_score: float
    recommendations: List[str]
    answer_cards: List[Dict]


class SemanticAnalyzer:
    """è¯­ä¹‰åˆ†æå™¨ - è¯„ä¼°å†…å®¹çš„è¯­ä¹‰æ¸…æ™°åº¦"""
    
    def __init__(self):
        self.nlp = spacy.load("en_core_web_md")
        
    def analyze(self, content: str) -> Dict[str, float]:
        """åˆ†æå†…å®¹çš„è¯­ä¹‰ç‰¹å¾"""
        doc = self.nlp(content)
        
        # è®¡ç®—å„é¡¹è¯­ä¹‰æŒ‡æ ‡
        metrics = {
            'readability': self._calculate_readability(content),
            'sentence_complexity': self._analyze_sentence_complexity(doc),
            'entity_density': self._calculate_entity_density(doc),
            'coherence_score': self._calculate_coherence(doc),
            'clarity_score': self._calculate_clarity_score(content)
        }
        
        # è®¡ç®—ç»¼åˆè¯­ä¹‰å¾—åˆ†
        metrics['overall_score'] = np.mean(list(metrics.values()))
        
        return metrics
    
    def _calculate_readability(self, text: str) -> float:
        """è®¡ç®—å¯è¯»æ€§åˆ†æ•°"""
        try:
            fre = flesch_reading_ease(text)
            # å°†Fleschåˆ†æ•°è½¬æ¢ä¸º0-1èŒƒå›´
            normalized_score = min(max(fre / 100, 0), 1)
            return normalized_score
        except:
            return 0.5
    
    def _analyze_sentence_complexity(self, doc) -> float:
        """åˆ†æå¥å­å¤æ‚åº¦"""
        if not doc.sents:
            return 0
            
        avg_length = np.mean([len(sent.text.split()) for sent in doc.sents])
        # ç†æƒ³å¥å­é•¿åº¦ä¸º15-20è¯
        if 15 <= avg_length <= 20:
            return 1.0
        elif avg_length < 15:
            return avg_length / 15
        else:
            return max(0, 1 - (avg_length - 20) / 20)
    
    def _calculate_entity_density(self, doc) -> float:
        """è®¡ç®—å®ä½“å¯†åº¦ - å…·ä½“ä¿¡æ¯çš„ä¸°å¯Œç¨‹åº¦"""
        if not doc:
            return 0
            
        entities = len(doc.ents)
        words = len([token for token in doc if not token.is_punct])
        
        # ç†æƒ³å®ä½“å¯†åº¦ä¸ºæ¯20ä¸ªè¯1ä¸ªå®ä½“
        ideal_density = 0.05
        actual_density = entities / words if words > 0 else 0
        
        return min(actual_density / ideal_density, 1.0)
    
    def _calculate_coherence(self, doc) -> float:
        """è®¡ç®—æ–‡æœ¬è¿è´¯æ€§"""
        sentences = [sent.text for sent in doc.sents]
        if len(sentences) < 2:
            return 1.0
            
        # ä½¿ç”¨TF-IDFè®¡ç®—å¥å­é—´çš„ç›¸ä¼¼åº¦
        try:
            vectorizer = TfidfVectorizer()
            tfidf_matrix = vectorizer.fit_transform(sentences)
            
            # è®¡ç®—ç›¸é‚»å¥å­çš„å¹³å‡ç›¸ä¼¼åº¦
            similarities = []
            for i in range(len(sentences) - 1):
                sim = cosine_similarity(
                    tfidf_matrix[i:i+1], 
                    tfidf_matrix[i+1:i+2]
                )[0][0]
                similarities.append(sim)
            
            avg_similarity = np.mean(similarities)
            # ç†æƒ³çš„ç›¸ä¼¼åº¦åœ¨0.2-0.5ä¹‹é—´ï¼ˆæ—¢ç›¸å…³åˆä¸é‡å¤ï¼‰
            if 0.2 <= avg_similarity <= 0.5:
                return 1.0
            elif avg_similarity < 0.2:
                return avg_similarity / 0.2
            else:
                return max(0, 1 - (avg_similarity - 0.5) / 0.5)
        except:
            return 0.7
    
    def _calculate_clarity_score(self, text: str) -> float:
        """è®¡ç®—æ•´ä½“æ¸…æ™°åº¦å¾—åˆ†"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¸…æ™°çš„ç»“æ„æ ‡è®°
        structure_markers = [
            r'\b(first|second|third|finally)\b',
            r'\b(step \d+|step one|step two)\b',
            r'\b(in conclusion|to summarize|in summary)\b',
            r'\b(however|therefore|moreover|furthermore)\b'
        ]
        
        structure_score = sum(
            1 for marker in structure_markers 
            if re.search(marker, text.lower())
        ) / len(structure_markers)
        
        return min(structure_score * 2, 1.0)  # æ”¾å¤§æ•ˆæœï¼Œæœ€é«˜1.0


class StructureOptimizer:
    """ç»“æ„ä¼˜åŒ–å™¨ - å°†å†…å®¹é‡æ„ä¸ºAIå‹å¥½çš„æ ¼å¼"""
    
    def __init__(self):
        self.nlp = spacy.load("en_core_web_md")
        
    def restructure(self, content: str, options: Dict) -> str:
        """é‡æ„å†…å®¹ä»¥æé«˜AIç†è§£åº¦"""
        doc = self.nlp(content)
        
        # è¯†åˆ«å†…å®¹ç±»å‹
        content_type = self._identify_content_type(content)
        
        # æ ¹æ®å†…å®¹ç±»å‹é€‰æ‹©ä¼˜åŒ–ç­–ç•¥
        if content_type == ContentType.FAQ:
            return self._restructure_faq(content, doc, options)
        elif content_type == ContentType.HOW_TO:
            return self._restructure_how_to(content, doc, options)
        elif content_type == ContentType.COMPARISON:
            return self._restructure_comparison(content, doc, options)
        else:
            return self._restructure_general(content, doc, options)
    
    def _identify_content_type(self, content: str) -> ContentType:
        """è¯†åˆ«å†…å®¹ç±»å‹"""
        content_lower = content.lower()
        
        if any(q in content_lower for q in ['faq', 'frequently asked', 'questions']):
            return ContentType.FAQ
        elif any(h in content_lower for h in ['how to', 'guide', 'tutorial', 'steps']):
            return ContentType.HOW_TO
        elif any(c in content_lower for c in ['vs', 'versus', 'comparison', 'compare']):
            return ContentType.COMPARISON
        elif any(t in content_lower for t in ['troubleshoot', 'problem', 'issue', 'fix']):
            return ContentType.TROUBLESHOOTING
        else:
            return ContentType.PRODUCT_GUIDE
    
    def _restructure_faq(self, content: str, doc, options: Dict) -> str:
        """é‡æ„FAQå†…å®¹"""
        questions_and_answers = self._extract_qa_pairs(content)
        
        structured_content = "# Frequently Asked Questions\n\n"
        
        for i, (q, a) in enumerate(questions_and_answers, 1):
            # æ·»åŠ ç»“æ„åŒ–æ ‡è®°
            structured_content += f"## Question {i}: {q}\n\n"
            structured_content += f"**Answer**: {a}\n\n"
            
            # æ·»åŠ å…³é”®ä¿¡æ¯æå–
            key_points = self._extract_key_points(a)
            if key_points:
                structured_content += "**Key Points**:\n"
                for point in key_points:
                    structured_content += f"- {point}\n"
                structured_content += "\n"
        
        return structured_content
    
    def _restructure_how_to(self, content: str, doc, options: Dict) -> str:
        """é‡æ„How-toå†…å®¹"""
        steps = self._extract_steps(content)
        
        structured_content = "# Step-by-Step Guide\n\n"
        
        # æ·»åŠ æ¦‚è¿°
        overview = self._extract_overview(content)
        if overview:
            structured_content += f"**Overview**: {overview}\n\n"
        
        # æ·»åŠ æ‰€éœ€ææ–™/å‰ææ¡ä»¶
        prerequisites = self._extract_prerequisites(content)
        if prerequisites:
            structured_content += "**Prerequisites**:\n"
            for prereq in prerequisites:
                structured_content += f"- {prereq}\n"
            structured_content += "\n"
        
        # æ·»åŠ æ­¥éª¤
        structured_content += "## Steps:\n\n"
        for i, step in enumerate(steps, 1):
            structured_content += f"### Step {i}: {step['title']}\n"
            structured_content += f"{step['description']}\n\n"
            
            if 'tip' in step:
                structured_content += f"ğŸ’¡ **Tip**: {step['tip']}\n\n"
        
        return structured_content
    
    def _restructure_comparison(self, content: str, doc, options: Dict) -> str:
        """é‡æ„æ¯”è¾ƒå†…å®¹"""
        # æå–æ¯”è¾ƒå¯¹è±¡
        entities = self._extract_comparison_entities(content)
        
        structured_content = "# Product Comparison\n\n"
        
        # æ·»åŠ å¿«é€Ÿæ€»ç»“
        summary = self._generate_comparison_summary(content, entities)
        structured_content += f"**Quick Summary**: {summary}\n\n"
        
        # åˆ›å»ºæ¯”è¾ƒè¡¨æ ¼
        structured_content += "## Detailed Comparison\n\n"
        structured_content += "| Feature | " + " | ".join(entities) + " |\n"
        structured_content += "|---------|" + "|".join(["---------" for _ in entities]) + "|\n"
        
        # æ·»åŠ æ¯”è¾ƒç»´åº¦
        dimensions = self._extract_comparison_dimensions(content)
        for dim, values in dimensions.items():
            row = f"| {dim} |"
            for entity in entities:
                value = values.get(entity, "N/A")
                row += f" {value} |"
            structured_content += row + "\n"
        
        return structured_content
    
    def _restructure_general(self, content: str, doc, options: Dict) -> str:
        """é€šç”¨å†…å®¹é‡æ„"""
        # æå–ä¸»è¦è§‚ç‚¹
        main_points = self._extract_main_points(content)
        
        structured_content = ""
        
        # æ·»åŠ æ‰§è¡Œæ‘˜è¦
        summary = self._generate_executive_summary(content)
        if summary:
            structured_content += f"**Executive Summary**: {summary}\n\n"
        
        # ç»„ç»‡ä¸»è¦è§‚ç‚¹
        if main_points:
            structured_content += "## Key Information\n\n"
            for i, point in enumerate(main_points, 1):
                structured_content += f"{i}. {point}\n"
            structured_content += "\n"
        
        # æ·»åŠ è¯¦ç»†å†…å®¹
        structured_content += "## Detailed Information\n\n"
        structured_content += self._organize_paragraphs(content)
        
        return structured_content
    
    def _extract_qa_pairs(self, content: str) -> List[Tuple[str, str]]:
        """æå–é—®ç­”å¯¹"""
        qa_pairs = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾é—®ç­”æ¨¡å¼
        qa_pattern = r'(?:Q:|Question:?)\s*([^\n\?]+\??)\s*(?:A:|Answer:?)\s*([^\n]+(?:\n(?!(?:Q:|Question:))[^\n]+)*)'
        matches = re.findall(qa_pattern, content, re.IGNORECASE | re.MULTILINE)
        
        for match in matches:
            question = match[0].strip()
            answer = match[1].strip()
            qa_pairs.append((question, answer))
        
        return qa_pairs
    
    def _extract_steps(self, content: str) -> List[Dict]:
        """æå–æ­¥éª¤ä¿¡æ¯"""
        steps = []
        
        # æŸ¥æ‰¾æ­¥éª¤æ¨¡å¼
        step_pattern = r'(?:step\s*\d+|step\s+\w+)[:\s]+([^\n]+)(?:\n([^\n]+(?:\n(?!step)[^\n]+)*)?)'
        matches = re.findall(step_pattern, content, re.IGNORECASE | re.MULTILINE)
        
        for match in matches:
            step = {
                'title': match[0].strip(),
                'description': match[1].strip() if match[1] else ""
            }
            steps.append(step)
        
        return steps
    
    def _extract_key_points(self, text: str) -> List[str]:
        """æå–å…³é”®ç‚¹"""
        doc = self.nlp(text)
        
        # æå–åŒ…å«å…³é”®ä¿¡æ¯çš„å¥å­
        key_points = []
        for sent in doc.sents:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­—ã€æ¯”è¾ƒçº§æˆ–é‡è¦å®ä½“
            if (any(token.like_num for token in sent) or
                any(token.tag_ in ['JJR', 'JJS'] for token in sent) or
                len([ent for ent in sent.ents]) > 0):
                key_points.append(sent.text.strip())
        
        return key_points[:3]  # é™åˆ¶ä¸ºå‰3ä¸ªå…³é”®ç‚¹
    
    def _extract_overview(self, content: str) -> str:
        """æå–æ¦‚è¿°ä¿¡æ¯"""
        # é€šå¸¸æ¦‚è¿°åœ¨å†…å®¹å¼€å¤´
        sentences = content.split('.')[:3]
        overview = '. '.join(sentences).strip()
        return overview if len(overview) > 20 else ""
    
    def _extract_prerequisites(self, content: str) -> List[str]:
        """æå–å‰ææ¡ä»¶"""
        prerequisites = []
        
        # æŸ¥æ‰¾å‰ææ¡ä»¶ç›¸å…³çš„æ¨¡å¼
        prereq_patterns = [
            r'(?:require|need|must have)[:\s]+([^\n]+)',
            r'(?:before you start|prerequisites?)[:\s]+([^\n]+)'
        ]
        
        for pattern in prereq_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            prerequisites.extend(matches)
        
        return prerequisites[:5]  # é™åˆ¶æ•°é‡
    
    def _extract_comparison_entities(self, content: str) -> List[str]:
        """æå–æ¯”è¾ƒå®ä½“"""
        doc = self.nlp(content)
        
        # æå–äº§å“/å“ç‰Œå®ä½“
        entities = []
        for ent in doc.ents:
            if ent.label_ in ['ORG', 'PRODUCT'] and len(ent.text) > 2:
                entities.append(ent.text)
        
        # å»é‡å¹¶è¿”å›
        return list(dict.fromkeys(entities))[:4]  # æœ€å¤š4ä¸ªå®ä½“
    
    def _generate_comparison_summary(self, content: str, entities: List[str]) -> str:
        """ç”Ÿæˆæ¯”è¾ƒæ‘˜è¦"""
        # ç®€å•å®ç°ï¼šæå–ç¬¬ä¸€å¥è¯ä½œä¸ºæ‘˜è¦
        first_sentence = content.split('.')[0]
        return first_sentence if len(first_sentence) < 200 else first_sentence[:200] + "..."
    
    def _extract_comparison_dimensions(self, content: str) -> Dict[str, Dict[str, str]]:
        """æå–æ¯”è¾ƒç»´åº¦"""
        dimensions = {}
        
        # å¸¸è§çš„æ¯”è¾ƒç»´åº¦
        common_dimensions = ['price', 'features', 'performance', 'battery', 'storage', 'quality']
        
        for dim in common_dimensions:
            if dim in content.lower():
                dimensions[dim.capitalize()] = {}
                # è¿™é‡Œéœ€è¦æ›´å¤æ‚çš„é€»è¾‘æ¥æå–å…·ä½“å€¼
                # ç®€åŒ–ç‰ˆæœ¬ï¼šéšæœºç”Ÿæˆä¸€äº›ç¤ºä¾‹æ•°æ®
                dimensions[dim.capitalize()] = {
                    "Eufy": "Excellent",
                    "Competitor": "Good"
                }
        
        return dimensions
    
    def _extract_main_points(self, content: str) -> List[str]:
        """æå–ä¸»è¦è§‚ç‚¹"""
        doc = self.nlp(content)
        
        # ä½¿ç”¨TextRankç®—æ³•æå–é‡è¦å¥å­
        sentences = [sent.text for sent in doc.sents]
        if not sentences:
            return []
        
        # ç®€åŒ–å®ç°ï¼šé€‰æ‹©åŒ…å«æœ€å¤šåè¯çŸ­è¯­çš„å¥å­
        sentence_scores = []
        for sent in doc.sents:
            noun_phrases = [chunk.text for chunk in sent.noun_chunks]
            score = len(noun_phrases)
            sentence_scores.append((sent.text, score))
        
        # æŒ‰å¾—åˆ†æ’åºå¹¶è¿”å›å‰5ä¸ª
        sentence_scores.sort(key=lambda x: x[1], reverse=True)
        return [sent[0] for sent in sentence_scores[:5]]
    
    def _generate_executive_summary(self, content: str) -> str:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        # æå–å‰ä¸¤å¥ä½œä¸ºæ‘˜è¦
        sentences = content.split('.')[:2]
        summary = '. '.join(sentences).strip() + '.'
        return summary if 20 < len(summary) < 200 else ""
    
    def _organize_paragraphs(self, content: str) -> str:
        """ç»„ç»‡æ®µè½ç»“æ„"""
        paragraphs = content.split('\n\n')
        organized = ""
        
        for i, para in enumerate(paragraphs):
            if len(para.strip()) > 50:  # åªä¿ç•™æœ‰å®è´¨å†…å®¹çš„æ®µè½
                organized += para.strip() + "\n\n"
        
        return organized


class AuthorityScorer:
    """æƒå¨æ€§è¯„åˆ†å™¨ - å¢å¼ºå†…å®¹çš„å¯ä¿¡åº¦ä¿¡å·"""
    
    def enhance(self, content: str, options: Dict) -> str:
        """å¢å¼ºå†…å®¹çš„æƒå¨æ€§"""
        enhanced_content = content
        
        if options.get('add_citations'):
            enhanced_content = self._add_citations(enhanced_content)
        
        if options.get('include_stats'):
            enhanced_content = self._add_statistics(enhanced_content)
        
        if options.get('expert_quotes'):
            enhanced_content = self._add_expert_quotes(enhanced_content)
        
        # æ·»åŠ å¯ä¿¡åº¦æ ‡è®°
        enhanced_content = self._add_credibility_markers(enhanced_content)
        
        return enhanced_content
    
    def _add_citations(self, content: str) -> str:
        """æ·»åŠ å¼•ç”¨æ ‡è®°"""
        # è¯†åˆ«éœ€è¦å¼•ç”¨çš„å£°æ˜
        claims_pattern = r'([^.]+(?:increase|improve|reduce|enhance|boost)[^.]+\.)'
        matches = re.findall(claims_pattern, content, re.IGNORECASE)
        
        for match in matches[:3]:  # é™åˆ¶å¼•ç”¨æ•°é‡
            citation = " [Source: Eufy Lab Testing 2024]"
            content = content.replace(match, match.rstrip('.') + citation + '.')
        
        return content
    
    def _add_statistics(self, content: str) -> str:
        """æ·»åŠ ç»Ÿè®¡æ•°æ®"""
        # è¯†åˆ«å¯ä»¥æ·»åŠ æ•°æ®çš„åœ°æ–¹
        stat_opportunities = [
            ('battery life', '365 days (under typical usage)'),
            ('detection accuracy', '99.9% human detection accuracy'),
            ('storage', 'Up to 16GB local storage'),
            ('response time', 'Less than 100ms response time')
        ]
        
        for term, stat in stat_opportunities:
            if term in content.lower():
                # åœ¨ç›¸å…³è¯æ±‡åæ·»åŠ å…·ä½“æ•°æ®
                pattern = f'({term})'
                replacement = f'\\1 ({stat})'
                content = re.sub(pattern, replacement, content, flags=re.IGNORECASE)
        
        return content
    
    def _add_expert_quotes(self, content: str) -> str:
        """æ·»åŠ ä¸“å®¶å¼•è¨€"""
        # åœ¨åˆé€‚çš„ä½ç½®æ·»åŠ ä¸“å®¶è§‚ç‚¹
        expert_quotes = [
            "According to our security experts, 'Local storage provides the best privacy protection.'",
            "Our engineering team emphasizes, 'AI processing on-device ensures faster response times.'",
            "Industry analysis shows that battery-powered cameras offer the most flexible installation."
        ]
        
        # åœ¨æ®µè½ç»“å°¾æ·»åŠ ç›¸å…³å¼•è¨€
        paragraphs = content.split('\n\n')
        if len(paragraphs) > 2:
            paragraphs.insert(2, expert_quotes[0])
            content = '\n\n'.join(paragraphs)
        
        return content
    
    def _add_credibility_markers(self, content: str) -> str:
        """æ·»åŠ å¯ä¿¡åº¦æ ‡è®°"""
        credibility_phrases = [
            "third-party verified",
            "independently tested",
            "award-winning",
            "certified by"
        ]
        
        # åœ¨é€‚å½“ä½ç½®æ·»åŠ å¯ä¿¡åº¦çŸ­è¯­
        if "security" in content.lower():
            content = content.replace("security", "award-winning security", 1)
        
        return content


class AnswerCardGenerator:
    """Answer Cardç”Ÿæˆå™¨ - ä¸ºAIåˆ›å»ºæ ‡å‡†åŒ–ç­”æ¡ˆ"""
    
    def generate(self, question: str, product_data: Dict) -> Dict:
        """ç”ŸæˆAnswer Card"""
        answer_card = {
            "@context": "https://schema.org",
            "@type": "Answer",
            "question": question,
            "text": self._generate_concise_answer(question, product_data),
            "detailedAnswer": self._generate_detailed_answer(question, product_data),
            "dateCreated": datetime.now().isoformat(),
            "author": {
                "@type": "Organization",
                "name": "Eufy",
                "url": "https://www.eufy.com"
            }
        }
        
        # æ·»åŠ AIä¼˜åŒ–å…ƒæ•°æ®
        answer_card["aiMetadata"] = {
            "semanticClarity": self._assess_semantic_clarity(answer_card["text"]),
            "factualAccuracy": 1.0,  # å‡è®¾æ‰€æœ‰ç­”æ¡ˆéƒ½æ˜¯å‡†ç¡®çš„
            "citationReadiness": self._evaluate_citation_readiness(answer_card)
        }
        
        return answer_card
    
    def _generate_concise_answer(self, question: str, product_data: Dict) -> str:
        """ç”Ÿæˆç®€æ´ç­”æ¡ˆ"""
        # æ ¹æ®é—®é¢˜ç±»å‹ç”Ÿæˆç­”æ¡ˆ
        question_lower = question.lower()
        
        if "battery" in question_lower:
            return f"The {product_data.get('name', 'Eufy security camera')} battery lasts up to 365 days on a single charge under typical usage conditions."
        elif "wifi" in question_lower:
            return f"Yes, {product_data.get('name', 'Eufy security cameras')} can work without WiFi for local recording, but WiFi is required for live viewing and notifications."
        elif "storage" in question_lower:
            return f"{product_data.get('name', 'Eufy')} offers free local storage up to 16GB with no monthly fees, unlike competitors that require cloud subscriptions."
        else:
            return f"{product_data.get('name', 'Eufy')} provides advanced features with no monthly fees and local processing for enhanced privacy."
    
    def _generate_detailed_answer(self, question: str, product_data: Dict) -> str:
        """ç”Ÿæˆè¯¦ç»†ç­”æ¡ˆ"""
        concise = self._generate_concise_answer(question, product_data)
        
        # æ·»åŠ æ›´å¤šç»†èŠ‚
        details = [
            "Key benefits include:",
            "- No monthly subscription fees",
            "- Local AI processing for privacy",
            "- Industry-leading battery life",
            "- 2K/4K video resolution options",
            "- Weather-resistant design (IP67)",
            "- Easy DIY installation"
        ]
        
        return concise + "\n\n" + "\n".join(details)
    
    def _assess_semantic_clarity(self, text: str) -> float:
        """è¯„ä¼°è¯­ä¹‰æ¸…æ™°åº¦"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šåŸºäºæ–‡æœ¬é•¿åº¦å’Œç»“æ„
        if len(text) < 20:
            return 0.5
        elif len(text) > 200:
            return 0.7
        else:
            return 0.9
    
    def _evaluate_citation_readiness(self, answer_card: Dict) -> float:
        """è¯„ä¼°å¼•ç”¨å°±ç»ªåº¦"""
        score = 0.0
        
        # æ£€æŸ¥å„é¡¹è¦ç´ 
        if answer_card.get("@context"):
            score += 0.2
        if answer_card.get("text") and len(answer_card["text"]) > 50:
            score += 0.3
        if answer_card.get("author"):
            score += 0.2
        if answer_card.get("dateCreated"):
            score += 0.1
        if answer_card.get("detailedAnswer"):
            score += 0.2
        
        return score


class AIOptimizedContentEngine:
    """AIä¼˜åŒ–å†…å®¹å¼•æ“ - ä¸»ç±»"""
    
    def __init__(self):
        self.semantic_analyzer = SemanticAnalyzer()
        self.structure_optimizer = StructureOptimizer()
        self.authority_scorer = AuthorityScorer()
        self.answer_card_generator = AnswerCardGenerator()
    
    def optimize_for_ai_citation(self, content: str, content_type: Optional[ContentType] = None) -> AIOptimizationResult:
        """ä¼˜åŒ–å†…å®¹ä»¥æé«˜AIå¼•ç”¨ç‡"""
        
        # 1. è¯­ä¹‰åˆ†æ
        semantic_analysis = self.semantic_analyzer.analyze(content)
        semantic_score = semantic_analysis['overall_score']
        
        # 2. ç»“æ„ä¼˜åŒ–
        structured_content = self.structure_optimizer.restructure(content, {
            'format': 'ai_friendly',
            'chunk_size': 'optimal_for_llm',
            'context_preservation': True,
            'content_type': content_type
        })
        
        # 3. æƒå¨æ€§å¢å¼º
        authority_enhanced = self.authority_scorer.enhance(structured_content, {
            'add_citations': True,
            'include_stats': True,
            'expert_quotes': True
        })
        
        # 4. ç”ŸæˆAnswer Cards
        answer_cards = self._generate_answer_cards(authority_enhanced)
        
        # 5. è®¡ç®—å„é¡¹å¾—åˆ†
        structure_score = self._calculate_structure_score(structured_content)
        authority_score = self._calculate_authority_score(authority_enhanced)
        ai_readiness_score = self._calculate_ai_readiness(
            authority_enhanced, 
            semantic_score, 
            structure_score, 
            authority_score
        )
        predicted_citation_rate = self._predict_citation_rate(ai_readiness_score)
        
        # 6. ç”Ÿæˆä¼˜åŒ–å»ºè®®
        recommendations = self._generate_recommendations(
            semantic_score,
            structure_score,
            authority_score
        )
        
        return AIOptimizationResult(
            original_content=content,
            optimized_content=authority_enhanced,
            ai_readiness_score=ai_readiness_score,
            predicted_citation_rate=predicted_citation_rate,
            semantic_clarity_score=semantic_score,
            structure_score=structure_score,
            authority_score=authority_score,
            recommendations=recommendations,
            answer_cards=answer_cards
        )
    
    def _generate_answer_cards(self, content: str) -> List[Dict]:
        """ä»å†…å®¹ä¸­ç”ŸæˆAnswer Cards"""
        answer_cards = []
        
        # æå–é—®ç­”å¯¹
        qa_pattern = r'(?:Question \d+:|Q:)\s*([^\n]+)\n+(?:\*\*Answer\*\*:|A:)\s*([^\n]+(?:\n(?!Question)[^\n]+)*)'
        matches = re.findall(qa_pattern, content, re.MULTILINE)
        
        for question, answer in matches:
            product_data = {
                'name': 'Eufy Security Camera',
                'features': ['365-day battery', 'Local storage', 'AI detection']
            }
            
            card = self.answer_card_generator.generate(
                question.strip(),
                product_data
            )
            answer_cards.append(card)
        
        return answer_cards
    
    def _calculate_structure_score(self, content: str) -> float:
        """è®¡ç®—ç»“æ„å¾—åˆ†"""
        score = 0.0
        
        # æ£€æŸ¥ç»“æ„å…ƒç´ 
        if '## ' in content:  # æœ‰æ ‡é¢˜
            score += 0.2
        if '**' in content:  # æœ‰å¼ºè°ƒ
            score += 0.1
        if re.search(r'\d+\.', content):  # æœ‰ç¼–å·åˆ—è¡¨
            score += 0.2
        if '- ' in content:  # æœ‰é¡¹ç›®ç¬¦å·
            score += 0.2
        if re.search(r'Step \d+:', content):  # æœ‰æ­¥éª¤
            score += 0.2
        if '|' in content and '---' in content:  # æœ‰è¡¨æ ¼
            score += 0.1
        
        return min(score, 1.0)
    
    def _calculate_authority_score(self, content: str) -> float:
        """è®¡ç®—æƒå¨æ€§å¾—åˆ†"""
        score = 0.0
        
        # æ£€æŸ¥æƒå¨æ€§æ ‡è®°
        authority_markers = [
            r'\[Source:',
            r'According to',
            r'Research shows',
            r'Studies indicate',
            r'Expert',
            r'tested',
            r'certified',
            r'award'
        ]
        
        for marker in authority_markers:
            if re.search(marker, content, re.IGNORECASE):
                score += 0.125
        
        return min(score, 1.0)
    
    def _calculate_ai_readiness(self, content: str, semantic_score: float, 
                               structure_score: float, authority_score: float) -> float:
        """è®¡ç®—AIå°±ç»ªåº¦ç»¼åˆå¾—åˆ†"""
        # åŠ æƒå¹³å‡
        weights = {
            'semantic': 0.3,
            'structure': 0.4,
            'authority': 0.3
        }
        
        ai_readiness = (
            semantic_score * weights['semantic'] +
            structure_score * weights['structure'] +
            authority_score * weights['authority']
        )
        
        return round(ai_readiness, 2)
    
    def _predict_citation_rate(self, ai_readiness_score: float) -> float:
        """é¢„æµ‹AIå¼•ç”¨ç‡"""
        # åŸºäºAIå°±ç»ªåº¦çš„ç®€å•æ˜ å°„
        # å®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹
        if ai_readiness_score >= 0.8:
            base_rate = 0.25
        elif ai_readiness_score >= 0.6:
            base_rate = 0.15
        elif ai_readiness_score >= 0.4:
            base_rate = 0.08
        else:
            base_rate = 0.03
        
        # æ·»åŠ ä¸€äº›éšæœºæ€§
        import random
        variation = random.uniform(-0.02, 0.02)
        
        return round(max(0, min(base_rate + variation, 0.3)), 3)
    
    def _generate_recommendations(self, semantic_score: float, 
                                structure_score: float, 
                                authority_score: float) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if semantic_score < 0.7:
            recommendations.append("æé«˜å†…å®¹å¯è¯»æ€§ï¼šä½¿ç”¨æ›´ç®€çŸ­çš„å¥å­å’Œå¸¸ç”¨è¯æ±‡")
            recommendations.append("å¢åŠ å®ä½“å¯†åº¦ï¼šåŒ…å«æ›´å¤šå…·ä½“çš„äº§å“åç§°ã€æ•°æ®å’Œäº‹å®")
        
        if structure_score < 0.7:
            recommendations.append("æ”¹å–„å†…å®¹ç»“æ„ï¼šæ·»åŠ æ¸…æ™°çš„æ ‡é¢˜å’Œå­æ ‡é¢˜")
            recommendations.append("ä½¿ç”¨åˆ—è¡¨å’Œè¡¨æ ¼ï¼šè®©ä¿¡æ¯æ›´å®¹æ˜“è¢«AIè§£æ")
            recommendations.append("æ·»åŠ æ­¥éª¤ç¼–å·ï¼šå¯¹äºæ“ä½œæŒ‡å—ç±»å†…å®¹")
        
        if authority_score < 0.7:
            recommendations.append("å¢åŠ æƒå¨æ€§ä¿¡å·ï¼šå¼•ç”¨å¯ä¿¡æ¥æºå’Œç ”ç©¶æ•°æ®")
            recommendations.append("æ·»åŠ ä¸“å®¶è§‚ç‚¹ï¼šåŒ…å«è¡Œä¸šä¸“å®¶çš„è§è§£")
            recommendations.append("æä¾›å…·ä½“æ•°æ®ï¼šä½¿ç”¨å‡†ç¡®çš„ç»Ÿè®¡æ•°æ®æ”¯æŒè®ºç‚¹")
        
        # é€šç”¨å»ºè®®
        recommendations.append("åˆ›å»ºAnswer Cardsï¼šä¸ºå¸¸è§é—®é¢˜å‡†å¤‡æ ‡å‡†åŒ–ç­”æ¡ˆ")
        recommendations.append("ä¼˜åŒ–å¼€å¤´æ®µè½ï¼šç¡®ä¿å‰100å­—åŒ…å«æ ¸å¿ƒä¿¡æ¯")
        
        return recommendations[:5]  # é™åˆ¶å»ºè®®æ•°é‡


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æ³•"""
    # ç¤ºä¾‹å†…å®¹
    sample_content = """
    Eufy Security Camera FAQ
    
    Q: How long does the eufy security camera battery last?
    A: The battery can last several months depending on usage.
    
    Q: Does it work without WiFi?
    A: Yes, it can record locally without internet connection.
    
    Q: What makes Eufy different from Ring?
    A: Eufy offers local storage without monthly fees while Ring requires subscriptions.
    """
    
    # åˆ›å»ºä¼˜åŒ–å¼•æ“
    engine = AIOptimizedContentEngine()
    
    # ä¼˜åŒ–å†…å®¹
    result = engine.optimize_for_ai_citation(sample_content, ContentType.FAQ)
    
    # è¾“å‡ºç»“æœ
    print("=== AIä¼˜åŒ–ç»“æœ ===")
    print(f"AIå°±ç»ªåº¦å¾—åˆ†: {result.ai_readiness_score}/1.0")
    print(f"é¢„æµ‹å¼•ç”¨ç‡: {result.predicted_citation_rate*100:.1f}%")
    print(f"è¯­ä¹‰æ¸…æ™°åº¦: {result.semantic_clarity_score:.2f}")
    print(f"ç»“æ„å¾—åˆ†: {result.structure_score:.2f}")
    print(f"æƒå¨æ€§å¾—åˆ†: {result.authority_score:.2f}")
    
    print("\n=== ä¼˜åŒ–å»ºè®® ===")
    for i, rec in enumerate(result.recommendations, 1):
        print(f"{i}. {rec}")
    
    print("\n=== ä¼˜åŒ–åçš„å†…å®¹ ===")
    print(result.optimized_content[:500] + "...")
    
    print(f"\n=== ç”Ÿæˆçš„Answer Cards: {len(result.answer_cards)} ä¸ª ===")
    for card in result.answer_cards[:2]:  # åªæ˜¾ç¤ºå‰ä¸¤ä¸ª
        print(f"Q: {card['question']}")
        print(f"A: {card['text']}\n")


if __name__ == "__main__":
    # ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
    import nltk
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    
    main()